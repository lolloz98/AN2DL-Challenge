{"cells":[{"cell_type":"markdown","metadata":{"id":"Hcx-ViOatPdF"},"source":["# Settings"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":746,"status":"ok","timestamp":1636985700480,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"enXV-EBXMA1-"},"outputs":[],"source":["# set to false if working on local machine\n","onColab = False\n","\n","epochs = 200\n","batch_size = 32\n","validation_split = 0.2\n","\n","# set to true if you need to unzip the data\n","doUnzipData = False\n","\n","# set to false to delete test folder and get ready to submit!\n","# set to true to use/create test folder\n","isTest = True\n","\n","# decide which model to train \n","# You can also set them all to true. They will be all trained\n","baseline = False\n","resnet50 = True\n","vgg16 = False\n","inceptionv_3 = True\n","\n","# PATH_TO_CHECKPOINTS is a folder which contains/will contain the checkpoints for the models. It is not necessary that it already exists\n","PATH_TO_CHECKPOINTS = './checkpoints/'"]},{"cell_type":"markdown","metadata":{"id":"2wYytTCYBqd1"},"source":["# Init\n","We initialize google drive, import the libraries, and initialize whatever *global* variable we need."]},{"cell_type":"markdown","metadata":{"id":"7rZJVcnKDr9M"},"source":["## Google drive access"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1636985701247,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Asj4wP02BHWM"},"outputs":[],"source":["if onColab:\n","  from google.colab import drive\n","  drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1636985701248,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Nck6i1SpCNyi"},"outputs":[],"source":["if onColab:\n","  %cd /gdrive/MyDrive/University/ANN/CHALLENGE1/\n","  !ls"]},{"cell_type":"markdown","metadata":{"id":"NHrU1oqyDxHw"},"source":["## Importing libs"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1636985701248,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"GN8BLO6yCfmC"},"outputs":[],"source":["import os\n","import time\n","import scipy\n","import random\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import tensorflow as tf\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2906,"status":"ok","timestamp":1636985704147,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"dY0-guNHDMKY","outputId":"a714cfe3-b8a8-42ff-b1e9-84305ee6177c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: visualkeras in c:\\users\\carpa\\desktop\\challenge\\venv\\lib\\site-packages (0.0.2)\n","Requirement already satisfied: numpy>=1.18.1 in c:\\users\\carpa\\desktop\\challenge\\venv\\lib\\site-packages (from visualkeras) (1.21.4)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\carpa\\desktop\\challenge\\venv\\lib\\site-packages (from visualkeras) (8.4.0)\n","Requirement already satisfied: aggdraw>=1.3.11 in c:\\users\\carpa\\desktop\\challenge\\venv\\lib\\site-packages (from visualkeras) (1.3.12)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: You are using pip version 21.1.1; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\users\\carpa\\desktop\\challenge\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"]}],"source":["!pip install visualkeras\n","import visualkeras"]},{"cell_type":"markdown","metadata":{"id":"0JNF7JcGD1ZZ"},"source":["## Setting seed"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1636985704147,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"T9FGuRfWHH7J"},"outputs":[],"source":["seed = 20"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","executionInfo":{"elapsed":9,"status":"ok","timestamp":1636985704148,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"SBIwZDZVDSy5"},"outputs":[],"source":["#@title init seed everywhere\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"LZ6aMxwqaqVd"},"source":["## Some parameters init"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1636985704148,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"pG-r5TnMG5qZ"},"outputs":[],"source":["labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']\n","\n","baseline_path_to_ckpt = PATH_TO_CHECKPOINTS + 'baseline/ckpts/cp.ckpt/'\n","resnet50_path_to_ckpt = PATH_TO_CHECKPOINTS + 'resnet50/ckpts/cp.ckpt/'\n","vgg16_path_to_ckpt = PATH_TO_CHECKPOINTS + 'vgg16/ckpts/cp.ckpt/'\n","inception_v3_path_to_ckpt = PATH_TO_CHECKPOINTS + 'inception_v3/ckpts/cp.ckpt/'\n","\n","dataset_dir = 'training'\n","dataset_path = './training'\n","\n","img_h, img_w = (256, 256)\n","input_shape = (img_h, img_w, 3)\n","\n","# check total number of files. It must be 17728 if no testset\n","def countInTraining():\n","  count = 0\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","    for j in range(len(class_imgs)):\n","      count += 1\n","  print(\"In training: \", count)\n","  return count"]},{"cell_type":"markdown","metadata":{"id":"F46LEeKAD8u3"},"source":["## Unzip dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"cellView":"form","executionInfo":{"elapsed":8,"status":"ok","timestamp":1636985704148,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"eD_TQmUPEBkx"},"outputs":[],"source":["#@title Run this cell to unzip data\n","if doUnzipData:\n","  !unzip dataset.zip"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1636985704148,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"-htvZpMMFqEB"},"outputs":[],"source":["#@title Plot example images from dataset\n","if doUnzipData:\n","  num_row = len(labels)//2\n","  num_col = len(labels)//num_row\n","  fig, axes = plt.subplots(num_row, num_col, figsize=(2*num_row,15*num_col))\n","  for i in range(len(labels)):\n","    if i < len(labels):\n","      class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","      class_img = class_imgs[0]\n","      print(class_imgs)\n","      # print(class_img)\n","      img = Image.open('{}/{}/{}'.format(dataset_dir, labels[i], class_img))\n","      ax = axes[i//num_col, i%num_col]\n","      ax.imshow(np.array(img))\n","      ax.set_title('{}'.format(labels[i]))\n","  plt.tight_layout()\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RTWA09wYCT2U"},"source":["## Only if evaluating the performance\n","If test folder is not present we create one. \n","\n","For each class type we generate a folder and we move 1/100 images of that type to that folder"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1636985704149,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"ND7tmLJqChau"},"outputs":[],"source":["test_path = \"./test\"\n","if isTest and not os.path.exists(test_path):\n","  import shutil\n","  os.mkdir(test_path)\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","    # In this way we always get the same data. \n","    # TODO: randomize using seed the data we take\n","    class_imgs.sort()\n","    # put in test 1/100 of the pictures of one folder\n","    for j in range(len(class_imgs) // 50):\n","      class_img = class_imgs[j]\n","      dest_dir = './test/' + labels[i] + '/'\n","      if not os.path.exists(dest_dir):\n","        os.mkdir(dest_dir)\n","      shutil.move('{}/{}/'.format(dataset_dir, labels[i]) + class_img, dest_dir)\n","\n","# countInTraining()"]},{"cell_type":"markdown","metadata":{"id":"S2OFIzGDCbSN"},"source":["## If we want to submit the model\n","We do not generate test folder. \n","\n","If test folder is present we move the images back into the train folder and delete the test folder"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1636985704149,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Z7Jpon-bGMCG"},"outputs":[],"source":["test_path = \"./test\"\n","if os.path.exists(test_path) and not isTest:\n","  import shutil\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(test_path, labels[i])))[2]\n","    # put back in training the picture in test\n","    for j in range(len(class_imgs)):\n","      class_img = class_imgs[j]\n","      dest_dir = './training/' + labels[i] + '/'\n","      shutil.move('{}/{}/'.format(test_path, labels[i]) + class_img, dest_dir)\n","  shutil.rmtree(test_path)\n","\n","#   if countInTraining() != 17728:\n","#     print(\"CAREFUL: some images could have been deleted\")\n","\n","# countInTraining()"]},{"cell_type":"markdown","metadata":{"id":"9fm9TaN1qmsj"},"source":["# Import data, preprocess, useful functions"]},{"cell_type":"markdown","metadata":{"id":"rS8_mJxkZhdF"},"source":["## Import data"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1636985704149,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"rqB_go3BgJE4"},"outputs":[],"source":["#@title Another possible way to load the dataset\n","# # One possible way to load the dataset. I use another which was used also in class, using ImageDataGenerator\n","# train_ds = tf.keras.utils.image_dataset_from_directory(\n","#   dataset_path,\n","#   validation_split=0.2,\n","#   subset=\"training\",\n","#   seed=seed,\n","#   image_size=(img_h, img_w),\n","#   batch_size=batch_size)\n","\n","# # Check sizes\n","# # train_ds is divided in batch of batch_size with images with shape (img_h, img_w, n_channels)\n","# for image_batch, labels_batch in train_ds:\n","#   print(image_batch.shape)\n","#   print(labels_batch.shape)\n","#   break\n","\n","# val_ds = tf.keras.utils.image_dataset_from_directory(\n","#   dataset_path,\n","#   validation_split=0.2,\n","#   subset=\"validation\",\n","#   seed=seed,\n","#   image_size=(img_h, img_w),\n","#   batch_size=batch_size)\n","\n","# # just trying to print some images in the train_ds\n","# plt.figure(figsize=(10, 10))\n","# for images, labels in train_ds.take(1):\n","#   for i in range(len(train_ds.class_names)):\n","#     ax = plt.subplot(4, 4, i + 1)\n","#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n","#     plt.title(train_ds.class_names[labels[i]])\n","#     plt.axis(\"off\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1636985704403,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"p7nWo4AjtwvU","outputId":"d593687e-86d0-4ac4-9346-b8881c0086ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 13911 images belonging to 14 classes.\n","Found 3469 images belonging to 14 classes.\n","Found 348 images belonging to 14 classes.\n"]}],"source":["# Images are divided into folders, one for each class. \n","# If the images are organized in such a way, we can exploit the \n","# ImageDataGenerator to read them from disk.\n","\n","# Create an instance of ImageDataGenerator for training and validation\n","# We also apply data augmentation\n","def load_data(dontUseFun=True, fun=None):\n","  image_generator = ImageDataGenerator(preprocessing_function = fun, validation_split=validation_split)  \n","  if dontUseFun:\n","    image_generator = ImageDataGenerator(rotation_range=30,\n","                                          height_shift_range=50,\n","                                          width_shift_range=50,\n","                                          zoom_range=0.3,\n","                                          horizontal_flip=True,\n","                                          vertical_flip=True, \n","                                          fill_mode='reflect',\n","                                          rescale=1./255, \n","                                          validation_split=0.2)  \n","\n","\n","  # Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","  train_gen = image_generator.flow_from_directory(directory=dataset_path,\n","                                                target_size=(img_h, img_w),\n","                                                color_mode='rgb',\n","                                                classes=labels, # can be set to None\n","                                                class_mode='categorical',\n","                                                subset='training',\n","                                                batch_size=batch_size,\n","                                                shuffle=True,\n","                                                seed=seed)\n","\n","  valid_gen = image_generator.flow_from_directory(directory=dataset_path,\n","                                                target_size=(img_h, img_w),\n","                                                color_mode='rgb',\n","                                                classes=labels, # can be set to None\n","                                                class_mode='categorical',\n","                                                subset='validation',\n","                                                batch_size=batch_size,\n","                                                shuffle=False,\n","                                                seed=seed)\n","  test_gen = None\n","  if isTest:\n","    test_image_gen = ImageDataGenerator(preprocessing_function = fun)\n","    if dontUseFun:\n","      test_image_gen = ImageDataGenerator(rescale=1./255)\n","    test_gen = test_image_gen.flow_from_directory(directory=test_path,\n","                                                target_size=(img_h, img_w),\n","                                                color_mode='rgb',\n","                                                classes=labels, # can be set to None\n","                                                class_mode='categorical',\n","                                                batch_size=batch_size,\n","                                                shuffle=False,\n","                                                seed=seed)\n","  return train_gen, valid_gen, test_gen\n","train_gen, valid_gen, test_gen = load_data()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704403,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"vBkCLhROyi9D"},"outputs":[],"source":["def get_next_batch(generator):\n","  batch = next(generator)\n","\n","  image = batch[0]\n","  target = batch[1]\n","\n","  print(\"(Input) image shape:\", image.shape)\n","  print(\"Target shape:\",target.shape)\n","\n","  # Visualize only the first sample\n","  image = image[0]\n","  target = target[0]\n","  target_idx = np.argmax(target)\n","  print()\n","  print(\"Categorical label:\", target)\n","  print(\"Label:\", target_idx)\n","  print(\"Class name:\", labels[target_idx])\n","  fig = plt.figure(figsize=(6, 4))\n","  gen = ImageDataGenerator()  \n","  plt.imshow(np.uint8(image * 255))\n","\n","  return batch\n","\n","# Get a sample from dataset and show info\n","# _ = get_next_batch(train_gen)"]},{"cell_type":"markdown","metadata":{"id":"d4CVGSkBE0zo"},"source":["## Model Comparison\n","Here we put functions to plot models against other models"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704404,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"zm8e1WYkEz7r"},"outputs":[],"source":["def compareModels(history1, h1lbl, history2=None, h2lbl=''):\n","  '''\n","  Plot history1 against history2. \n","  If history2 is None then Plot twice history1 (it'll be overlapped)\n","  h1lbl and h2lbl are the labels of the two histories\n","  '''\n","  if history2 == None: \n","    history2 = history1\n","    h2lbl = h1lbl\n","  # Plot the training\n","  plt.figure(figsize=(15,5))\n","  plt.plot(history1['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n","  plt.plot(history1['val_loss'], label=h1lbl, alpha=.8, color='#ff7f0e')\n","  plt.plot(history2['loss'], alpha=.3, color='#4D61E2', linestyle='--')\n","  plt.plot(history2['val_loss'], label=h2lbl, alpha=.8, color='#4D61E2')\n","  plt.legend(loc='upper left')\n","  plt.title('Categorical Crossentropy')\n","  plt.grid(alpha=.3)\n","\n","  plt.figure(figsize=(15,5))\n","  plt.plot(history1['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n","  plt.plot(history1['val_accuracy'], label=h1lbl, alpha=.8, color='#ff7f0e')\n","  plt.plot(history2['accuracy'], alpha=.3, color='#4D61E2', linestyle='--')\n","  plt.plot(history2['val_accuracy'], label=h2lbl, alpha=.8, color='#4D61E2')\n","  plt.legend(loc='upper left')\n","  plt.title('Accuracy')\n","  plt.grid(alpha=.3)\n","\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S-YlqQ83frjM"},"source":["## Callbacks & checkpoints\n","Tired of colab disconnections? Me too, let's save checkpoints while training :)\n","\n","Here there is also early stopping"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704404,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"rw56wFAafqoX"},"outputs":[],"source":["# Utility function to create folders and callbacks for training\n","from datetime import datetime\n","\n","def create_folders_and_callbacks(model_name, ckpt_path=PATH_TO_CHECKPOINTS, overridePrev = True):\n","\n","  if not os.path.exists(ckpt_path):\n","      os.makedirs(ckpt_path)\n","      \n","  now = ''\n","  if not overridePrev: now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","  exp_dir = os.path.join(ckpt_path, model_name + '_' + str(now))\n","  if now == '': exp_dir = os.path.join(ckpt_path, model_name)\n","\n","  if not os.path.exists(exp_dir):\n","      os.makedirs(exp_dir)\n","      \n","  callbacks = []\n","\n","  # Model checkpoint\n","  # ----------------\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","  if not os.path.exists(ckpt_dir):\n","      os.makedirs(ckpt_dir)\n","\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n","                                                     save_weights_only=False, # True to save only weights\n","                                                     save_best_only=True) # True to save only the best epoch. \n","                                                                           # We use early stopping, thus, in this way we can save both the last and the best\n","  callbacks.append(ckpt_callback)\n","\n","  # Visualize Learning on Tensorboard\n","  # ---------------------------------\n","  tb_dir = os.path.join(exp_dir, 'tb_logs')\n","  if not os.path.exists(tb_dir):\n","      os.makedirs(tb_dir)\n","      \n","  # By default shows losses and metrics for both training and validation\n","  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n","                                               profile_batch=0,\n","                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n","  callbacks.append(tb_callback)\n","\n","  return callbacks\n","\n","def callbacks_with_es(model_name, ckpt_path=PATH_TO_CHECKPOINTS, overridePrev = True):\n","  callbacks = create_folders_and_callbacks(model_name, ckpt_path, overridePrev)\n","  # Early Stopping\n","  # --------------\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","  callbacks.append(es_callback)\n","  return callbacks\n","\n","def retrieveCheckpoint(path_to_ckps):\n","  try:\n","    return tf.keras.models.load_model(path_to_ckps)\n","  except:\n","    # TODO: better exception handling\n","    return None"]},{"cell_type":"markdown","metadata":{"id":"1qgqi7JxHvi_"},"source":["# Baseline model\n","I just use the model proposed in class"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704668,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"sWOysHer0334"},"outputs":[],"source":["# Model used for the exercise:\n","# (Conv + ReLU + MaxPool) x 5 + FC x 2\n","def build_baseline_model(input_shape):\n","\n","    # Build the neural network layer by layer\n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","\n","    conv1 = tfkl.Conv2D(\n","        filters=16,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(input_layer)\n","    pool1 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv1)\n","\n","    conv2 = tfkl.Conv2D(\n","        filters=32,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool1)\n","    pool2 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv2)\n","\n","    conv3 = tfkl.Conv2D(\n","        filters=64,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool2)\n","    pool3 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv3)\n","\n","    conv4 = tfkl.Conv2D(\n","        filters=128,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool3)\n","    pool4 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv4)\n","\n","    conv5 = tfkl.Conv2D(\n","        filters=256,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool4)\n","    pool5 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv5)\n","\n","    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n","    flattening_layer = tfkl.Dropout(0.3, seed=seed)(flattening_layer)\n","    classifier_layer = tfkl.Dense(units=512, name='Classifier', kernel_initializer=tfk.initializers.GlorotUniform(seed), activation='relu')(flattening_layer)\n","    classifier_layer = tfkl.Dropout(0.3, seed=seed)(classifier_layer)\n","    output_layer = tfkl.Dense(units=14, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='Output')(classifier_layer)\n","\n","    # Connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","    # Compile the model\n","    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","    # Return the model\n","    return model"]},{"cell_type":"code","execution_count":19,"metadata":{"cellView":"form","executionInfo":{"elapsed":6,"status":"ok","timestamp":1636985704669,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"6qbixQiprQl_"},"outputs":[],"source":["#@title This is declared above with changes (Keep it also here commented out)\n","\n","# # Utility function to create folders and callbacks for training\n","# from datetime import datetime\n","\n","# def create_folders_and_callbacks(model_name):\n","\n","#   exps_dir = os.path.join('experiments')\n","#   if not os.path.exists(exps_dir):\n","#       os.makedirs(exps_dir)\n","\n","#   now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","#   exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","#   if not os.path.exists(exp_dir):\n","#       os.makedirs(exp_dir)\n","      \n","#   callbacks = []\n","\n","#   # Model checkpoint\n","#   # ----------------\n","#   ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","#   if not os.path.exists(ckpt_dir):\n","#       os.makedirs(ckpt_dir)\n","\n","#   ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n","#                                                      save_weights_only=False, # True to save only weights\n","#                                                      save_best_only=False) # True to save only the best epoch. \n","#                                                                            # We use early stopping, thus, in this way we can save both the last and the best\n","#   callbacks.append(ckpt_callback)\n","\n","#   # Visualize Learning on Tensorboard\n","#   # ---------------------------------\n","#   tb_dir = os.path.join(exp_dir, 'tb_logs')\n","#   if not os.path.exists(tb_dir):\n","#       os.makedirs(tb_dir)\n","      \n","#   # By default shows losses and metrics for both training and validation\n","#   tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n","#                                                profile_batch=0,\n","#                                                histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n","#   callbacks.append(tb_callback)\n","\n","#   # Early Stopping\n","#   # --------------\n","#   es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","#   callbacks.append(es_callback)\n","\n","#   return callbacks"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704669,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"6Wz8Y6AMzyyk"},"outputs":[],"source":["# Build model (for data augmentation training)\n","if baseline:\n","  model = retrieveCheckpoint(baseline_path_to_ckpt)\n","  if model == None:\n","    model = build_baseline_model(input_shape)\n","  model.summary()"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1636985704670,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"pXAHAuoSz-_S"},"outputs":[],"source":["tf.get_logger().setLevel('WARNING') #  if you want to suppress only INFOs\n","# tf.get_logger().setLevel('ERROR') #  if you want to suppress both WARNINGs and INFOs\n","\n","# Create folders and callbacks and fit\n","aug_callbacks = callbacks_with_es(model_name='baseline')\n","\n","if baseline:\n","  # Train the model\n","  history = model.fit(\n","      x = train_gen,\n","      epochs = epochs,\n","      validation_data = valid_gen,\n","      callbacks = aug_callbacks,\n","  ).history"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1636985704670,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"IwhA2qPP0Nvg"},"outputs":[],"source":["# Save best epoch model\n","if baseline:\n","  model.save(\"experimets/baseline_best\")"]},{"cell_type":"markdown","metadata":{"id":"MVtSgmIb51a6"},"source":["# Resnet50\n","Transfer learning on Resnet50"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1636985704901,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"-mga9Ltq58-e","outputId":"0f476ce0-8ccd-430f-bf16-ec4d197c3adc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 13911 images belonging to 14 classes.\n","Found 3469 images belonging to 14 classes.\n","Found 348 images belonging to 14 classes.\n"]}],"source":["if resnet50:\n","  from tensorflow.keras.applications.resnet50 import preprocess_input\n","\n","  # reload data without preprocessing\n","  train_gen, valid_gen, test_gen = load_data(False, preprocess_input)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9183,"status":"ok","timestamp":1636985714082,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"0769eXUHA662","outputId":"ca6a8734-650f-45a6-82ab-8d6e225902a7"},"outputs":[],"source":["if resnet50:\n","  supernet = tfk.applications.ResNet50(\n","      include_top=False, # we remove the classifier\n","      weights=\"imagenet\",\n","      input_shape=(img_h * 2, img_w * 2, 3)\n","  )\n","  # supernet.summary()\n","  # tfk.utils.plot_model(supernet)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1096,"status":"ok","timestamp":1636985715165,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Wme_WCVcBssE","outputId":"7c92affc-1802-4af6-fee9-9362ca75848b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 256, 256, 3)]     0         \n","                                                                 \n"," resizing (Resizing)         (None, 512, 512, 3)       0         \n","                                                                 \n"," resnet50 (Functional)       (None, 16, 16, 2048)      23587712  \n","                                                                 \n"," Flattening (Flatten)        (None, 524288)            0         \n","                                                                 \n"," dropout (Dropout)           (None, 524288)            0         \n","                                                                 \n"," dense (Dense)               (None, 256)               134217984 \n","                                                                 \n"," dropout_1 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 14)                3598      \n","                                                                 \n","=================================================================\n","Total params: 157,809,294\n","Trainable params: 134,221,582\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["if resnet50:\n","  supernet.trainable = False # we want to train only the classifier, not the feature extractor\n","\n","  # Now we use the supernet as layer of our network\n","  inputs = tfk.Input(shape=(img_h, img_w, 3))\n","  x = tfkl.Resizing(img_h * 2, img_w * 2, interpolation=\"bicubic\")(inputs)\n","  x = supernet(x)\n","\n","  # Now we build our classifier (which will be actually trained)\n","  x = tfkl.Flatten(name='Flattening')(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  x = tfkl.Dense(\n","      256, \n","      activation='relu',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  outputs = tfkl.Dense(\n","      len(labels), \n","      activation='softmax',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","\n","  # Look if there is a checkpoint\n","  res_model = retrieveCheckpoint(resnet50_path_to_ckpt)\n","  if res_model == None:\n","    print('No checkpoint found, creating new model')\n","    # Connect input and output through the Model class\n","    res_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n","\n","    # Compile the model\n","    res_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","  res_model.summary()"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"elapsed":256,"status":"error","timestamp":1636985735997,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"-kCAw6tiDWRG","outputId":"3888adfb-9045-4cbc-d7a3-40940ac4c000"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\carpa\\Desktop\\Challenge\\venv\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/200\n"," 11/435 [..............................] - ETA: 1:48:55 - loss: 1.0263 - accuracy: 0.7131"]}],"source":["if resnet50:\n","  callbacks = create_folders_and_callbacks(model_name='resnet50')\n","  callbacks.append(tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True))\n","  res_history = res_model.fit(\n","      x = train_gen,\n","      batch_size = batch_size,\n","      epochs = epochs,\n","      validation_data = valid_gen,\n","      callbacks = callbacks\n","  ).history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1636985715166,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"gmJQ3b-vGphD"},"outputs":[],"source":["if resnet50:\n","  res_model.save(\"./resnet50_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1636985715166,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"huSUGOG6Dwxc"},"outputs":[],"source":["if resnet50:\n","    compareModels(res_history, 'resnet50')"]},{"cell_type":"markdown","metadata":{"id":"2EYchCDskMyk"},"source":["# VGG16\n","Transfer learning with fine tuning on VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1636985715166,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"jDSQ3OtEkMyl"},"outputs":[],"source":["if vgg16:\n","  from tensorflow.keras.applications.vgg16 import preprocess_input\n","\n","  # reload data without preprocessing\n","  train_gen, valid_gen, test_gen = load_data(False, preprocess_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1636985715167,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"gg1eraR_kMyl"},"outputs":[],"source":["if vgg16:\n","  supernet = tfk.applications.VGG16(\n","      include_top=False, # we remove the classifier\n","      weights=\"imagenet\",\n","      input_shape=(img_h * 2, img_w * 2, 3)\n","  )\n","  # supernet.summary()\n","  # tfk.utils.plot_model(supernet)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1636985715167,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"VlPbaKLOkMym"},"outputs":[],"source":["if vgg16:\n","  # Freeze first N layers, e.g., until 14th\n","  for i, layer in enumerate(supernet.layers[:14]):\n","    layer.trainable=False\n","  for i, layer in enumerate(supernet.layers):\n","    print(i, layer.name, layer.trainable)\n","  # supernet.summary()\n","\n","  # Now we use the supernet as layer of our network\n","  inputs = tfk.Input(shape=(img_h, img_w, 3))\n","  x = tfkl.Resizing(img_h * 2, img_w * 2, interpolation=\"bicubic\")(inputs)\n","  x = supernet(x)\n","\n","  # Now we build our classifier (which will be actually trained)\n","  x = tfkl.Flatten(name='Flattening')(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  x = tfkl.Dense(\n","      256, \n","      activation='relu',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  outputs = tfkl.Dense(\n","      len(labels), \n","      activation='softmax',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","    \n","  # Look if there is a checkpoint\n","  vgg_model = retrieveCheckpoint(vgg16_path_to_ckpt)\n","  if vgg_model == None:\n","    # Connect input and output through the Model class\n","    vgg_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n","\n","    # Compile the model\n","    vgg_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","  vgg_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"aborted","timestamp":1636985715167,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"tssSnqs8kMym"},"outputs":[],"source":["if vgg16:\n","  callbacks = create_folders_and_callbacks(model_name='vgg16')\n","  callbacks.append(tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True))\n","  vgg_history = vgg_model.fit(\n","      x = train_gen,\n","      batch_size = batch_size,\n","      epochs = epochs,\n","      validation_data = valid_gen,\n","      callbacks = callbacks\n","  ).history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1636985715168,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"O0gnbnUakMyt"},"outputs":[],"source":["if vgg16:\n","  vgg_model.save(\"./vgg16_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1636985715168,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"B5UYczYPkMyt"},"outputs":[],"source":["if vgg16:\n","    compareModels(vgg_history, 'vgg16')"]},{"cell_type":"markdown","metadata":{"id":"o4ZN-0VgT7kJ"},"source":["# InceptionV_3\n","Transfer learning on InecptionV_3"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1636985715168,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Vvo26w20T7kN"},"outputs":[],"source":["if inceptionv_3:\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","\n","  # reload data without preprocessing\n","  train_gen, valid_gen, test_gen = load_data(False, preprocess_input)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1636985715169,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"fpzZ_WkFT7kO"},"outputs":[],"source":["if inceptionv_3:\n","  supernet = tfk.applications.InceptionV3(\n","      include_top=False, # we remove the classifier\n","      weights=\"imagenet\",\n","      input_shape=(img_h * 2, img_w * 2, 3)\n","  )\n","  # supernet.summary()\n","  # tfk.utils.plot_model(supernet)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1636985715169,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"Xm7YOFesT7kO"},"outputs":[],"source":["if inceptionv_3:\n","  # # Freeze first N layers, e.g., until 14th\n","  # for i, layer in enumerate(supernet.layers[:14]):\n","  #   layer.trainable=False\n","  # for i, layer in enumerate(supernet.layers):\n","  #    print(i, layer.name, layer.trainable)\n","  # # supernet.summary()\n","\n","  supernet.trainable = False\n","\n","  # Now we use the supernet as layer of our network\n","  inputs = tfk.Input(shape=(img_h, img_w, 3))\n","  x = tfkl.Resizing(img_h * 2, img_w * 2, interpolation=\"bicubic\")(inputs)\n","  x = supernet(x)\n","\n","  # Now we build our classifier (which will be actually trained)\n","  x = tfkl.Flatten(name='Flattening')(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  x = tfkl.Dense(\n","      256, \n","      activation='relu',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","  x = tfkl.Dropout(0.3, seed=seed)(x)\n","  outputs = tfkl.Dense(\n","      len(labels), \n","      activation='softmax',\n","      kernel_initializer = tfk.initializers.GlorotUniform(seed))(x)\n","      \n","  \n","  # Look if there is a checkpoint\n","  inc_model = retrieveCheckpoint(vgg16_path_to_ckpt)\n","  if inc_model == None:\n","    # Connect input and output through the Model class\n","    inc_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n","\n","    # Compile the model\n","    inc_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","  inc_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":303,"status":"aborted","timestamp":1636985715463,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"-rXr_RldT7kP"},"outputs":[],"source":["if inceptionv_3:\n","  callbacks = create_folders_and_callbacks(model_name='inceptionv_3')\n","  callbacks.append(tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=10, restore_best_weights=True))\n","  inc_history = inc_model.fit(\n","      x = train_gen,\n","      batch_size = batch_size,\n","      epochs = epochs,\n","      validation_data = valid_gen,\n","      callbacks = callbacks\n","  ).history"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":303,"status":"aborted","timestamp":1636985715463,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"y2ba-8D_T7kP"},"outputs":[],"source":["if inceptionv_3:\n","  inc_model.save(\"./inceptionv_3_best\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":304,"status":"aborted","timestamp":1636985715464,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"},"user_tz":-60},"id":"CI8ILaviT7kP"},"outputs":[],"source":["\n","if inceptionv_3:\n","    inc_model(inc_history, 'incv_3')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["NHrU1oqyDxHw","0JNF7JcGD1ZZ","d4CVGSkBE0zo"],"name":"carpa.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
