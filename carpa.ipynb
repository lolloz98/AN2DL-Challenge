{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Challenge.ipynb","provenance":[],"collapsed_sections":["0JNF7JcGD1ZZ"],"authorship_tag":"ABX9TyNZTn+fAgEVumDSrczc99bn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2wYytTCYBqd1"},"source":["# Init\n","We initialize google drive, import the libraries, and initialize whatever *global* variable we need."]},{"cell_type":"code","metadata":{"id":"enXV-EBXMA1-","executionInfo":{"status":"ok","timestamp":1636907704868,"user_tz":-60,"elapsed":200,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["# set to true if you need to unzip the data\n","doUnzipData = False\n","# set to false to delete test folder and get ready to submit!\n","isTest = True\n","\n","# decide which model to run\n","baseline = True"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rZJVcnKDr9M"},"source":["## Google drive access"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Asj4wP02BHWM","executionInfo":{"status":"ok","timestamp":1636907705718,"user_tz":-60,"elapsed":364,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"6bf33977-6561-4114-dc22-8f0784b9d163"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nck6i1SpCNyi","executionInfo":{"status":"ok","timestamp":1636907705719,"user_tz":-60,"elapsed":10,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"e60e5ad5-48f3-4c05-fb04-89d2e9077464"},"source":["%cd /gdrive/MyDrive/University/ANN/CHALLENGE1/"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/University/ANN/CHALLENGE1\n"]}]},{"cell_type":"markdown","metadata":{"id":"NHrU1oqyDxHw"},"source":["## Importing libs"]},{"cell_type":"code","metadata":{"id":"GN8BLO6yCfmC","executionInfo":{"status":"ok","timestamp":1636907705719,"user_tz":-60,"elapsed":7,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["import os\n","import time\n","import random\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import tensorflow as tf\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dY0-guNHDMKY","executionInfo":{"status":"ok","timestamp":1636907708901,"user_tz":-60,"elapsed":3188,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"9594810e-4790-49f1-bc22-18a4217128d1"},"source":["!pip install visualkeras\n","import visualkeras"],"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: visualkeras in /usr/local/lib/python3.7/dist-packages (0.0.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from visualkeras) (7.1.2)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from visualkeras) (1.19.5)\n","Requirement already satisfied: aggdraw>=1.3.11 in /usr/local/lib/python3.7/dist-packages (from visualkeras) (1.3.12)\n"]}]},{"cell_type":"markdown","metadata":{"id":"0JNF7JcGD1ZZ"},"source":["## Setting seed"]},{"cell_type":"code","metadata":{"id":"T9FGuRfWHH7J","executionInfo":{"status":"ok","timestamp":1636907708902,"user_tz":-60,"elapsed":7,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["seed = 20"],"execution_count":73,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"SBIwZDZVDSy5","executionInfo":{"status":"ok","timestamp":1636907708902,"user_tz":-60,"elapsed":6,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["#@title init seed everywhere\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZ6aMxwqaqVd"},"source":["## Some parameters init"]},{"cell_type":"code","metadata":{"id":"pG-r5TnMG5qZ","executionInfo":{"status":"ok","timestamp":1636907708902,"user_tz":-60,"elapsed":5,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']\n","dataset_dir = 'training'\n","dataset_path = './training'\n","\n","img_h, img_w = (256, 256)\n","input_shape = (img_h, img_w, 3)\n","epochs = 200\n","batch_size = 32\n","\n","# check total number of files. It must be 17728 if no testset\n","def countInTraining():\n","  count = 0\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","    for j in range(len(class_imgs)):\n","      count += 1\n","  print(\"In training: \", count)\n","  return count"],"execution_count":75,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F46LEeKAD8u3"},"source":["## Unzip dataset"]},{"cell_type":"code","metadata":{"id":"eD_TQmUPEBkx","cellView":"form","executionInfo":{"status":"ok","timestamp":1636907708903,"user_tz":-60,"elapsed":6,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["#@title Run this cell to unzip data\n","if doUnzipData:\n","  !unzip dataset.zip"],"execution_count":76,"outputs":[]},{"cell_type":"code","metadata":{"id":"-htvZpMMFqEB","cellView":"form","executionInfo":{"status":"ok","timestamp":1636907708903,"user_tz":-60,"elapsed":6,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["#@title Plot example images from dataset\n","if doUnzipData:\n","  num_row = len(labels)//2\n","  num_col = len(labels)//num_row\n","  fig, axes = plt.subplots(num_row, num_col, figsize=(2*num_row,15*num_col))\n","  for i in range(len(labels)):\n","    if i < len(labels):\n","      class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","      class_img = class_imgs[0]\n","      print(class_imgs)\n","      # print(class_img)\n","      img = Image.open('{}/{}/{}'.format(dataset_dir, labels[i], class_img))\n","      ax = axes[i//num_col, i%num_col]\n","      ax.imshow(np.array(img))\n","      ax.set_title('{}'.format(labels[i]))\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":77,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RTWA09wYCT2U"},"source":["## Only if evaluating the performance\n","If test folder is not present we create one. \n","\n","For each class type we generate a folder and we move 1/100 images of that type to that folder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ND7tmLJqChau","executionInfo":{"status":"ok","timestamp":1636907709214,"user_tz":-60,"elapsed":316,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"a8ada2e2-d0f4-4e2c-8902-98d2d188f309"},"source":["test_path = \"./test\"\n","if isTest and not os.path.exists(test_path):\n","  import shutil\n","  os.mkdir(test_path)\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(dataset_dir, labels[i])))[2]\n","    # In this way we always get the same data. \n","    # TODO: randomize using seed the data we take\n","    class_imgs.sort()\n","    # put in test 1/100 of the pictures of one folder\n","    for j in range(len(class_imgs) // 100):\n","      class_img = class_imgs[j]\n","      dest_dir = './test/' + labels[i] + '/'\n","      if not os.path.exists(dest_dir):\n","        os.mkdir(dest_dir)\n","      shutil.move('{}/{}/'.format(dataset_dir, labels[i]) + class_img, dest_dir)\n","\n","countInTraining()"],"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["In training:  17559\n"]},{"output_type":"execute_result","data":{"text/plain":["17559"]},"metadata":{},"execution_count":78}]},{"cell_type":"markdown","metadata":{"id":"S2OFIzGDCbSN"},"source":["## If we want to submit the model\n","We do not generate test folder. \n","\n","If test folder is present we move the images back into the train folder and delete the test folder"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7Jpon-bGMCG","executionInfo":{"status":"ok","timestamp":1636907709711,"user_tz":-60,"elapsed":499,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"4b57823d-8e9f-4de1-f69e-57822707d63b"},"source":["test_path = \"./test\"\n","if os.path.exists(test_path) and not isTest:\n","  import shutil\n","  for i in range(len(labels)):\n","    class_imgs = next(os.walk('{}/{}/'.format(test_path, labels[i])))[2]\n","    # put back in training the picture in test\n","    for j in range(len(class_imgs)):\n","      class_img = class_imgs[j]\n","      dest_dir = './training/' + labels[i] + '/'\n","      shutil.move('{}/{}/'.format(test_path, labels[i]) + class_img, dest_dir)\n","  shutil.rmtree(test_path)\n","\n","  if countInTraining() != 17728:\n","    print(\"CAREFUL: some images could have been deleted\")\n","\n","countInTraining()"],"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["In training:  17559\n"]},{"output_type":"execute_result","data":{"text/plain":["17559"]},"metadata":{},"execution_count":79}]},{"cell_type":"markdown","metadata":{"id":"rS8_mJxkZhdF"},"source":["# Import data"]},{"cell_type":"code","metadata":{"id":"rqB_go3BgJE4","cellView":"form","executionInfo":{"status":"ok","timestamp":1636907709711,"user_tz":-60,"elapsed":2,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["#@title Another possible way to load the dataset\n","# # One possible way to load the dataset. I use another which was used also in class, using ImageDataGenerator\n","# train_ds = tf.keras.utils.image_dataset_from_directory(\n","#   dataset_path,\n","#   validation_split=0.2,\n","#   subset=\"training\",\n","#   seed=seed,\n","#   image_size=(img_h, img_w),\n","#   batch_size=batch_size)\n","\n","# # Check sizes\n","# # train_ds is divided in batch of batch_size with images with shape (img_h, img_w, n_channels)\n","# for image_batch, labels_batch in train_ds:\n","#   print(image_batch.shape)\n","#   print(labels_batch.shape)\n","#   break\n","\n","# val_ds = tf.keras.utils.image_dataset_from_directory(\n","#   dataset_path,\n","#   validation_split=0.2,\n","#   subset=\"validation\",\n","#   seed=seed,\n","#   image_size=(img_h, img_w),\n","#   batch_size=batch_size)\n","\n","# # just trying to print some images in the train_ds\n","# plt.figure(figsize=(10, 10))\n","# for images, labels in train_ds.take(1):\n","#   for i in range(len(train_ds.class_names)):\n","#     ax = plt.subplot(4, 4, i + 1)\n","#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n","#     plt.title(train_ds.class_names[labels[i]])\n","#     plt.axis(\"off\")"],"execution_count":80,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7nWo4AjtwvU","executionInfo":{"status":"ok","timestamp":1636907711162,"user_tz":-60,"elapsed":1453,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"e7c5d830-de74-4e9a-da38-0063434feb34"},"source":["# Images are divided into folders, one for each class. \n","# If the images are organized in such a way, we can exploit the \n","# ImageDataGenerator to read them from disk.\n","\n","# Create an instance of ImageDataGenerator for training and validation\n","# We also apply data augmentation\n","image_generator = ImageDataGenerator(rotation_range=30,\n","                                      height_shift_range=50,\n","                                      width_shift_range=50,\n","                                      zoom_range=0.3,\n","                                      horizontal_flip=True,\n","                                      vertical_flip=True, \n","                                      fill_mode='reflect',\n","                                      rescale=1./255, \n","                                      validation_split=0.2)    \n","\n","\n","# Obtain a data generator with the 'ImageDataGenerator.flow_from_directory' method\n","train_gen = image_generator.flow_from_directory(directory=dataset_path,\n","                                               target_size=(img_h, img_w),\n","                                               color_mode='rgb',\n","                                               classes=labels, # can be set to None\n","                                               class_mode='categorical',\n","                                               subset='training',\n","                                               batch_size=batch_size,\n","                                               shuffle=True,\n","                                               seed=seed)\n","\n","valid_gen = image_generator.flow_from_directory(directory=dataset_path,\n","                                               target_size=(img_h, img_w),\n","                                               color_mode='rgb',\n","                                               classes=labels, # can be set to None\n","                                               class_mode='categorical',\n","                                               subset='validation',\n","                                               batch_size=batch_size,\n","                                               shuffle=False,\n","                                               seed=seed)\n","test_gen = None\n","if isTest:\n","  test_image_gen = ImageDataGenerator(rescale=1./255)\n","  test_gen = test_image_gen.flow_from_directory(directory=test_path,\n","                                               target_size=(img_h, img_w),\n","                                               color_mode='rgb',\n","                                               classes=labels, # can be set to None\n","                                               class_mode='categorical',\n","                                               batch_size=batch_size,\n","                                               shuffle=False,\n","                                               seed=seed)"],"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 14055 images belonging to 14 classes.\n","Found 3504 images belonging to 14 classes.\n","Found 169 images belonging to 14 classes.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"id":"vBkCLhROyi9D","executionInfo":{"status":"ok","timestamp":1636907712028,"user_tz":-60,"elapsed":868,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"6c38c902-8397-45d0-9819-ce1f102f8da3"},"source":["def get_next_batch(generator):\n","  batch = next(generator)\n","\n","  image = batch[0]\n","  target = batch[1]\n","\n","  print(\"(Input) image shape:\", image.shape)\n","  print(\"Target shape:\",target.shape)\n","\n","  # Visualize only the first sample\n","  image = image[0]\n","  target = target[0]\n","  target_idx = np.argmax(target)\n","  print()\n","  print(\"Categorical label:\", target)\n","  print(\"Label:\", target_idx)\n","  print(\"Class name:\", labels[target_idx])\n","  fig = plt.figure(figsize=(6, 4))\n","  gen = ImageDataGenerator()  \n","  plt.imshow(np.uint8(image))\n","\n","  return batch\n","\n","# Get a sample from dataset and show info\n","_ = get_next_batch(train_gen)"],"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["(Input) image shape: (32, 256, 256, 3)\n","Target shape: (32, 14)\n","\n","Categorical label: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","Label: 0\n","Class name: Apple\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"1qgqi7JxHvi_"},"source":["# Baseline model\n","I just use the model proposed in class"]},{"cell_type":"code","metadata":{"id":"sWOysHer0334","executionInfo":{"status":"ok","timestamp":1636907712029,"user_tz":-60,"elapsed":13,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["# Model used for the exercise:\n","# (Conv + ReLU + MaxPool) x 5 + FC x 2\n","def build_model(input_shape):\n","\n","    # Build the neural network layer by layer\n","    input_layer = tfkl.Input(shape=input_shape, name='Input')\n","\n","    conv1 = tfkl.Conv2D(\n","        filters=16,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(input_layer)\n","    pool1 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv1)\n","\n","    conv2 = tfkl.Conv2D(\n","        filters=32,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool1)\n","    pool2 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv2)\n","\n","    conv3 = tfkl.Conv2D(\n","        filters=64,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool2)\n","    pool3 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv3)\n","\n","    conv4 = tfkl.Conv2D(\n","        filters=128,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool3)\n","    pool4 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv4)\n","\n","    conv5 = tfkl.Conv2D(\n","        filters=256,\n","        kernel_size=(3, 3),\n","        strides = (1, 1),\n","        padding = 'same',\n","        activation = 'relu',\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed)\n","    )(pool4)\n","    pool5 = tfkl.MaxPooling2D(\n","        pool_size = (2, 2)\n","    )(conv5)\n","\n","    flattening_layer = tfkl.Flatten(name='Flatten')(pool5)\n","    flattening_layer = tfkl.Dropout(0.3, seed=seed)(flattening_layer)\n","    classifier_layer = tfkl.Dense(units=512, name='Classifier', kernel_initializer=tfk.initializers.GlorotUniform(seed), activation='relu')(flattening_layer)\n","    classifier_layer = tfkl.Dropout(0.3, seed=seed)(classifier_layer)\n","    output_layer = tfkl.Dense(units=14, activation='softmax', kernel_initializer=tfk.initializers.GlorotUniform(seed), name='Output')(classifier_layer)\n","\n","    # Connect input and output through the Model class\n","    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model')\n","\n","    # Compile the model\n","    model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","\n","    # Return the model\n","    return model"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qbixQiprQl_","executionInfo":{"status":"ok","timestamp":1636907712030,"user_tz":-60,"elapsed":12,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}}},"source":["# Utility function to create folders and callbacks for training\n","from datetime import datetime\n","\n","def create_folders_and_callbacks(model_name):\n","\n","  exps_dir = os.path.join('data_augmentation_experiments')\n","  if not os.path.exists(exps_dir):\n","      os.makedirs(exps_dir)\n","\n","  now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","  exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n","  if not os.path.exists(exp_dir):\n","      os.makedirs(exp_dir)\n","      \n","  callbacks = []\n","\n","  # Model checkpoint\n","  # ----------------\n","  ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","  if not os.path.exists(ckpt_dir):\n","      os.makedirs(ckpt_dir)\n","\n","  ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp.ckpt'), \n","                                                     save_weights_only=False, # True to save only weights\n","                                                     save_best_only=False) # True to save only the best epoch. \n","                                                                           # We use early stopping, thus, in this way we can save both the last and the best\n","  callbacks.append(ckpt_callback)\n","\n","  # Visualize Learning on Tensorboard\n","  # ---------------------------------\n","  tb_dir = os.path.join(exp_dir, 'tb_logs')\n","  if not os.path.exists(tb_dir):\n","      os.makedirs(tb_dir)\n","      \n","  # By default shows losses and metrics for both training and validation\n","  tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n","                                               profile_batch=0,\n","                                               histogram_freq=1)  # if > 0 (epochs) shows weights histograms\n","  callbacks.append(tb_callback)\n","\n","  # Early Stopping\n","  # --------------\n","  es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","  callbacks.append(es_callback)\n","\n","  return callbacks"],"execution_count":84,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Wz8Y6AMzyyk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636907712031,"user_tz":-60,"elapsed":13,"user":{"displayName":"Lorenzo Carpaneto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09045548220494827984"}},"outputId":"7806dbe7-9ca9-48e8-b5bd-be6aa3669400"},"source":["# Build model (for data augmentation training)\n","model = build_model(input_shape)\n","model.summary()"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Input (InputLayer)          [(None, 256, 256, 3)]     0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 256, 256, 16)      448       \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 128, 128, 16)     0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 128, 128, 32)      4640      \n","                                                                 \n"," max_pooling2d_6 (MaxPooling  (None, 64, 64, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 64, 64, 64)        18496     \n","                                                                 \n"," max_pooling2d_7 (MaxPooling  (None, 32, 32, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_8 (Conv2D)           (None, 32, 32, 128)       73856     \n","                                                                 \n"," max_pooling2d_8 (MaxPooling  (None, 16, 16, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_9 (Conv2D)           (None, 16, 16, 256)       295168    \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 8, 8, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," Flatten (Flatten)           (None, 16384)             0         \n","                                                                 \n"," dropout_2 (Dropout)         (None, 16384)             0         \n","                                                                 \n"," Classifier (Dense)          (None, 512)               8389120   \n","                                                                 \n"," dropout_3 (Dropout)         (None, 512)               0         \n","                                                                 \n"," Output (Dense)              (None, 14)                7182      \n","                                                                 \n","=================================================================\n","Total params: 8,788,910\n","Trainable params: 8,788,910\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXAHAuoSz-_S","outputId":"a288c0f2-43b2-4378-d44d-b17867580ea6"},"source":["tf.get_logger().setLevel('WARNING') #  if you want to suppress only INFOs\n","# tf.get_logger().setLevel('ERROR') #  if you want to suppress both WARNINGs and INFOs\n","\n","# Create folders and callbacks and fit\n","aug_callbacks = create_folders_and_callbacks(model_name='CNN_Aug')\n","\n","if baseline:\n","  # Train the model\n","  history = model.fit(\n","      x = train_gen,\n","      epochs = epochs,\n","      validation_data = valid_gen,\n","      callbacks = aug_callbacks,\n","  ).history"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n"," 15/440 [>.............................] - ETA: 17:09 - loss: 2.4146 - accuracy: 0.2521"]}]},{"cell_type":"code","metadata":{"id":"IwhA2qPP0Nvg"},"source":["# Save best epoch model\n","if baseline:\n","  model.save(\"experimets/baseline_best\")"],"execution_count":null,"outputs":[]}]}